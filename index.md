## Tópicos de investigación I (Machine Learning)


### Lecturas iniciales:

* El artículo [How Software in Half of NYC Cabs Generates $5.2 Million a Year in Extra Tips](http://iquantny.tumblr.com/post/107245431809/how-software-in-half-of-nyc-cabs-generates-52) sobre un ejemplo de analisis exploratorio. 

* La visualización y su importancia se encuentra en este artículo:[Anscombe’s Quartet, and Why Summary Statistics Don’t Tell the Whole Story - Heap Blog](https://blog.heapanalytics.com/anscombes-quartet-and-why-summary-statistics-dont-tell-the-whole-story/).

* Video acerca de la visualización y el uso de datos:

<iframe width="480" height="360" src="http://www.youtube.com/embed/coNDCIMH8bk"> </iframe> 

* Videos sobre paradigmas de aprendizaje (obligatorio): [http://work.caltech.edu/library/014.html](http://work.caltech.edu/library/014.html).

* Andrew ng y sus consejos en ML en el artículo : [ML-advice](http://cs229.stanford.edu/materials/ML-advice.pdf).

* Una comparación empírica de aprendizajes supervisados [Which Supervised Learning Method Works Best for What? An Empirical Comparison of Learning Methods and Metrics](http://www.cs.cornell.edu/%7Ecaruana/ctp/ct.papers/caruana.icml06.pdf).

* El artículo [Comparing supervised learning algorithms](http://www.dataschool.io/comparing-supervised-learning-algorithms/) muestra una comparación completa de aprendizajes supervisados.

* El mapa del ML según sckit-learn [aquí](http://scikit-learn.org/stable/tutorial/machine_learning_map).

* Algo de ingeniería características:

<iframe width="480" height="360" src="http://www.youtube.com/embed/bL4b1sGnILU"> </iframe> 

* Demasiado importante: [Learning from the best](http://blog.kaggle.com/2014/08/01/learning-from-the-best/).

* Principales usos de la ciencia de datos en [10 interesting ways to use data science](https://blog.dominodatalab.com/10-interesting-uses-of-data-science/).

### Libro de referencia

* David Barber, [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online), , Cambridge University Press, 2017. 

###  Material de referencias

* Otros libros de Machine Learning.
  - Python Machine Learning de Sebastian Raschka, Pack Publishing, 2015.
  - [Machine Learning: a Probabilistic Perspective](http://www.cs.ubc.ca/%7Emurphyk/MLbook/index.html) de Kevin Murphy (2012).
  - [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/prml/) de Chris Bishop  (2006). 
  - Data Science From Scratch: First Principles with Python de Joel Grus 2015.
  - Machine Learning refined: Foundations, Algorithms, and Applications 1st Edition Jeremy Watt, Reza Borhani y Aggelos K. Katsaggelos, 2016.
  - Foundations of Machine Learning (Adaptive Computation and Machine Learning series) Mehryar Mohri, Afshin Rostamizadeh  y Ameet Talwalkar, 2012.
  - Shai Shalev-Shwartz, and Shai Ben-David,[Understanding Machine Learning: From Theory to Algorithms](http://www.cs.huji.ac.il/%7Eshais/UnderstandingMachineLearning/index.html), Cambridge University Press, 2014.
 

* Cálculo
  - Apendice D del libro de Chris Bishop.
  - [Notas](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/readings/lagrange.pdf) del MIT para multiplicadores de Lagrange.
  - [Lagrange Multipliers without Permanent Scarring](https://people.eecs.berkeley.edu/~klein/papers/lagrange-multipliers.pdf) de  Dan Klein.
  
* Probabilidad
  
  - [Notas ](http://www.statslab.cam.ac.uk/~rrw1/prob/prob-weber.pdf) de  Richard Weber.
  - Capitulo 2 del libro de Kevin P. Murphy o Chris Bishop.
  - [Notas](http://cs229.stanford.edu/section/cs229-prob.pdf) de probabilidades de las clases de Machine Learning de Stanford.
 
* Álgebra Lineal
  - [Coding The Matrix: Linear Algebra Through Computer Science Applications](http://codingthematrix.com/), fantástico libro de Philip Klein (Revisar los diapositivas que acompañan al libro).
  - [Notas](http://cs229.stanford.edu/section/cs229-linalg.pdf) de álgebra lineal de las clases de Machine Learning de Stanford.
  - Apendice C del libro de Chris Bishop.
  - [Notas ](http://cs.nyu.edu/%7Edsontag/courses/ml12/notes/linear_algebra.pdf) de Sam Roweis.
  
* Optimización
  - [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) de Stephen Boyd y  Lieven Vandenberghe.
  - Notas de Optimización de las clases de Machine Learning de Stanford:
    * [Convex Optimization Overview 1](http://cs229.stanford.edu/section/cs229-cvxopt.pdf).
    * [Convex Optimization Overview 2](http://cs229.stanford.edu/section/cs229-cvxopt2.pdf).


### El mundo del machine learning y la IA

* Ejemplo 1: [Introduction](https://www.youtube.com/watch?v=i8D90DkCLhI).
* Ejemplo 2: [Rules on Rules on Rules](https://www.youtube.com/watch?v=2ZhQkD1QKFw).
* Ejemplo 3: [Now I R1](https://www.youtube.com/watch?v=0cRXaORbIFA).
* Ejemplo 4: [Machine Learning](https://www.youtube.com/watch?v=sarVw-iVWgc).
* Ejemplo 5: [To Learn is to Generalize](https://www.youtube.com/watch?v=efR8ybG7Ihs).
* Ejemplo 6: [It's Definitely Time to Play with Legos](https://www.youtube.com/watch?v=GufQYkMkdpw).
* Ejemplo 7: [There is no f](https://www.youtube.com/watch?v=klWUOO4sHaA).
* Ejemplo 8: [More Assumptions...Fewer Problems?](https://www.youtube.com/watch?v=UVwwYZMFocg).
* Ejemplo 9: [Bias Variance Throwdown](https://www.youtube.com/watch?v=ZYjCIazhKbk).
* Ejemplo 10: [World Domination](https://www.youtube.com/watch?v=6cvPj9dmYTo).
* Ejemplo 11: [Haystacks on Haystacks](https://www.youtube.com/watch?v=biy2yU3Auc4).
* Ejemplo 12: [Let's Get Greedy](https://www.youtube.com/watch?v=Kg8W_q8pHik).
* Ejemplo 13: [Heuristics](https://www.youtube.com/watch?v=g_sA8hYU3b8).
* Ejemplo 14: [Mejorando las Heurísticas](https://www.youtube.com/watch?v=tPHImr2sFBM).
* Ejemplo 15: [Information](https://www.youtube.com/watch?v=FMCY3SXTELE).


## Temario

* Clase 1: Introducción al Machine Learning 
    - Lectura obligatoria: [Capitulo1 de Kevin P. Murphy](http://www.cs.ubc.ca/%7Emurphyk/MLbook/pml-intro-22may12.pdf).
    - Cuaderno de Raúl Lopez Briega sobre una introducción a la [estadística y probabilidad con Python](https://nbviewer.jupyter.org/github/C-Lara/M-L/blob/master/pythonStats.ipynb).
    - Cuaderno introductorio de [scikit-learn](https://nbviewer.jupyter.org/github/C-Lara/M-L/blob/master/Cuadernos/scikit-learn/scikit-learn.ipynb).
    - [Diapositiva 1](https://github.com/C-Lara/M-L/blob/master/Diapositivas/Diapositiva1/Diapositiva1.pdf).
* Clase 2: Introducción al aprendizaje: Funciones de pérdida, algoritmo del perceptrón, prueba de errores del perceptrón.
    -  Cuaderno sobre [OSL](https://nbviewer.jupyter.org/github/C-Lara/M-L/blob/master/Cuadernos/OSL/OLS.ipynb).
    -  Cuaderno introductorio sobre [regresión lineal](https://nbviewer.jupyter.org/github/C-Lara/M-L/blob/master/Cuadernos/Regresion-lineal/Regresion-lineal.ipynb).
    - [The Perceptron, and All the Things it can't Perceive](https://jeremykun.com/2011/08/11/the-perceptron-and-all-the-things-it-cant-perceive/).
    - Lectura del capítulo 17  sobre regresión de mínimos cuadrados de  David Barber, [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online).
    - Lectura recomendada: [Perceptron Algorithm](http://www.jennwv.com/courses/F10/material/notes_1018.pdf).
    - Cuaderno introductorio sobre el [perceptron](https://nbviewer.jupyter.org/github/C-Lara/M-L/blob/master/Cuadernos/Perceptron/Perceptron.ipynb).
    - [Diapositiva 2](https://github.com/C-Lara/M-L/blob/master/Diapositivas/Diapositiva2/Diapositiva2.pdf).
* Clase 3: Clasificadores lineales: Introducción al SVM.
    - Lecturas previas:
      * [Convex functions](https://www.seas.ucla.edu/~vandenbe/ee236b/lectures/functions.pdf).
      * [Subgradients](http://stanford.edu/class/ee364b/lectures/subgradients_slides.pdf).
      * [Conjugate Functions](http://www.seas.ucla.edu/~vandenbe/236C/lectures/conj.pdf).
      * [Bregman Divergences](http://mark.reid.name/blog/meet-the-bregman-divergences.html).
    - [Support Vector Machine  ](http://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/).
    - Cuaderno introductorio de [SVM](https://nbviewer.jupyter.org/github/C-Lara/M-L/blob/master/Cuadernos/SVM/SVM.ipynb).
    - [Diapositiva 3]().
* Clase 4: SVM: Introducción a la optimización convexa
  - [Gradient, Subgradient and how they may affect your grade](http://cs.nyu.edu/~dsontag/courses/ml16/slides/notes_convexity16.pdf)
  - Lectura obligatoria: [Support Vector Machines](http://cs229.stanford.edu/notes/cs229-notes3.pdf) de Andrew Ng.
  - Notas adicionales: [Linear Support Vector Machines](https://davidrosenberg.github.io/ml2015/docs/svm-notes.pdf) de David S. Rosenberg.
  - [Diapositiva 4]().
* Clase 5: Descenso de gradiente estocástico
  - [Diapositiva 5]().
  
* Clase 6: Métodos del Kernel: Métodos del Kernel para SVM y classificación multiclases.
  - [Notas](http://cs229.stanford.edu/notes/cs229-notes3.pdf) sobre Kernels (sección 7).
  - [Pegasos Algorithm y  Kernels](http://cs.nyu.edu/~dsontag/courses/ml16/slides/lecture6_notes.pdf).
  - [Shalev-Shwartz & Ben-David](http://www.cs.huji.ac.il/%7Eshais/UnderstandingMachineLearning/index.html), capítulo 16 sobre métodos de kernels. Secciones 17.1 & 17.2 sobre multiclases.
  - [Diapositiva 6]().
 
* Clase 7: Regularización L1 y introducción a la teoria del aprendizaje
  - [Feature selection,L1 vs.L2 regularization, and rotational invariance](http://www.machinelearning.org/proceedings/icml2004/papers/354.pdf).
  - [Diapositiva 7]().
  
* Clase 8: Teoria del aprendizaje: Clases de hipótesis finitas
  - [Notas](http://cs229.stanford.edu/notes/cs229-notes4.pdf) de teoria del aprendizaje de Andrew Ng. Secciones 1 -3.
  - [Diapositiva 8]().
  
* Clase 9: Teoria de aprendizaje: Dimensión VC
  - [Notas](http://cs229.stanford.edu/notes/cs229-notes4.pdf) de teoria del aprendizaje de Andrew Ng. Sección 4.
  - Lectura opcional: [A Tutorial on Support Vector Machines for PatternRecognition](http://cs.nyu.edu/%7Edsontag/courses/ml14/notes/burges_SVM_tutorial.pdf). Páginas 29-31.
  - [Diapositiva 9]().

* Clase 10 : Árboles de decisión
  - [Trees](https://jeremykun.com/2012/09/16/trees-a-primer/).
  - [Decision trees ](https://jeremykun.com/2012/10/08/decision-trees-and-political-party-classification/).
  - [Tom M. Mitchel: Capitulo 3](http://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf).
  - Lectura opcional: [Cynthia Rudin: Decision Trees](https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec08.pdf).
  - [Diapositiva 10]().
  
* Clase 11: K-means
  - [k-Means Clustering ](https://jeremykun.com/2013/02/04/k-means-clustering-and-birth-rates/).
  - [Shalev-Shwartz & Ben-David](http://www.cs.huji.ac.il/%7Eshais/UnderstandingMachineLearning/index.html). Capítulo 22 (intro) y la sección 22.2.
  - [Diapositiva 11]().
  
* Clase 12: Clasificación jerárquica y espectral

  - [Shalev-Shwartz & Ben-David](http://www.cs.huji.ac.il/%7Eshais/UnderstandingMachineLearning/index.html). Secciones 22.1 y 22.3.
  - Lectura opcional: [A Tutorial on Spectral Clustering](http://cs.nyu.edu/%7Edsontag/courses/ml14/notes/Luxburg07_tutorial_spectral_clustering.pdf).
  - [Diapositiva 12]().
  
* Clase 13: Introducción a la inferencia Bayesiana: Naive Bayes.
  - [How Bayesian Inference works](http://www.kdnuggets.com/2016/11/how-bayesian-inference-works.html).
  - Artículo de Sebastian Raschka [Naive Bayes and Text Classification](http://sebastianraschka.com/Articles/2014_naive_bayes_1.html) sobre Naive Bayes.
  - [ Naives Bayes explained](http://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/).
  - [Generative and discriminative classifiers:Naives Bayes y Logistic Regression](http://www.cs.cmu.edu/%7Etom/mlbook/NBayesLogReg.pdf). Secciones 1 y 2.
  - [Shalev-Shwartz & Ben-David](http://www.cs.huji.ac.il/%7Eshais/UnderstandingMachineLearning/index.html). Sección 24.
  - [Diapositiva 13]().
  
* Clase 14: Regresión logística
  - [Logistic Regression - Interpreting Parameters](http://www.unm.edu/%7Eschrader/biostat/bio2/Spr06/lec11.pdf).
  - [Diapositiva 14]().
  
* Clase 15: K vecinos más cércanos
  - [A Detailed Introduction to K-Nearest Neighbor (KNN) Algorithm](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/).
  - Aplicaciones:
    * [Nearest Neighbor Retrieval and Classification ](http://vlm1.uta.edu/%7Eathitsos/nearest_neighbors/).
    * [Mapping Forest Resources Using the k-Nearest Neighbor Method](http://land.umn.edu/documents/FS6.pdf).
    * [Text categorization using k-nearest neighbor classification](http://user.ceng.metu.edu.tr/~e120321/paper.pdf).
  - [Diapositiva 15]().
  
## Evaluaciones en el curso CM-072

### Lista de temas propuestos

Los artículos propuestos se encuentran en la carpeta **PapersML** del repositorio del trabajo. Los participantes pueden elegir algún tema de proyecto de los artículos presentados o elegir alguno de su interés.

La fuente de muchos artículos relacionado al curso  es: [https://papers.nips.cc/](https://papers.nips.cc/) o [http://jmlr.csail.mit.edu/papers/](http://jmlr.csail.mit.edu/papers/).


### Fechas de presentaciones

Un cronograma propuesto de asignaciones es :

* Asignación 1: 29 de setiembre
* Asignacion 2: 17 de  octubre
* Asignacion 3: 1 de noviembre
* Asignacion 4: 11 de noviembre
* Asignacion 5: 23 de noviembre
* Asignación 6: 3 de diciembre

Fecha de presentación de proyectos

* Proyecto parcial: Semana del 9 al 13 de octubre
* Proyecto final: Semana del 11 al 15 de diciembre


Cualquier cambio que realices en tus  repositorios o documentos en  github después de la fecha de vencimiento se ignorará. Por favor, tengan  todo su trabajo enviado y probado (presentaciones y/o páginas web, screencasts, código etc.) antes de la fecha límite.

## Temas de trabajo 

1 .[Binarized Neural Networks](https://papers.nips.cc/paper/6573-binarized-neural-networks.pdf): Sobre un método para entrenar Redes Neuronales Binarizadas (BNNs) - redes neuronales con pesos binarios y activaciones en tiempo de ejecución. 

  * Frank  Cano Diaz
  * Franz Maguiña Aranda
  * Julio Cesar Silva
  * Gianfranco Ferro Palomino

Fecha de presentación : Jueves 19 de octubre de 6 a 9 pm.

2 .[A Neural Algorithm of Artistic Style](https://arxiv.org/pdf/1508.06576.pdf): El paper introduce un sistema artificial basado en una Red Neural Profunda que crea imágenes artísticas de alta calidad perceptiva.

  * Briggette Román Huaytalla
  * Danny Julca Carhuaz
  * Edgar Huaranga Junco 
  * Rogger Valverde Flores
  * Victor Sotelo Chico
  
3 .[Weight Normalization: A Simple Reparameterization
to Accelerate Training of Deep Neural Networks](https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf): Se presenta  una reparametrización de los vectores de pesos en una red neural que desacopla la longitud de esos vectores de peso desde su dirección.

  * Leonardo León Vera
  * Enrique Peinado Rodriguez
  * Renzo Cerrón Tome
  * Gerson Garrido Mansilla
  
  
  
4 .[Where are they looking?](http://carlvondrick.com/gaze.pdf) El paper presenta un modelo de  red profunda capaz de descubrir cómo extraer la postura de la cabeza y la orientación de la mirada, y seleccionar objetos en la escena que están en la línea de visión prevista.
 
  * Bruno Bedon Vasquez
  * Miguel Mejia Puma
  * Bitzer Arotoma Bacilio
  * Óscar Huarcaya Canal
 Fecha de presentación : Jueves 19 de octubre de 6 a 9 pm.
 
5 .[Map-Reduce for Machine Learning on Multicore](https://papers.nips.cc/paper/3150-map-reduce-for-machine-learning-on-multicore.pdf) El artículo muestra  que los algoritmos que se ajustan al modelo de consulta estadística pueden ser escritos en una cierta  *forma de suma*, que les permite ser fácilmente paralelizados en computadoras multinúcleo.

Se adapta el paradigma de reducción de mapas de Google para demostrar esta técnica de aceleración paralela en una variedad de algoritmos de aprendizaje, incluyendo regresión lineal ponderada localmente (LWLR), k-means, regresión logística (LR), Bayes naïve (NB), SVM, ICA, PCA , análisis discriminante gaussiano (GDA), EM y retropropagación (NN).

  * George Prado Acuña
  * Miguel Mini Huambachano
  * Victor Galvan Oyola

####  Equipo de proyecto

Las tareas, asignaciones y proyecto se realizarán con 4 compañeros de clase. En general, no anticipamos que las calificaciones para cada miembro del grupo serán diferentes, sin embargo, me reservo el derecho de asignar diferentes calificaciones a cada miembro del grupo, de acuerdo al grado de participación.

####  Código

Escribe todos tus  códigos y ejemplos de preferencia en  Python utilizando las librerias mencionadas en el curso como [scikit-learn](http://scikit-learn.org/) o [TensorFlow](https://www.tensorflow.org/), el código debe  las cosas de  manera correcta, pensando  en aspectos como la reutilización, manejo de errores, etc. Es menester que  documente  su código con herramientas como [doxygen](http://www.stack.nl/~dimitri/doxygen/).

####  Sitio web de las asignaciones y proyectos

El equipo debe presentar un sitio web público para sus asignaciones, tareas de reforzamiento, proyectos, utilizando  [Github Pages](https://pages.github.com/) o cualquier otro servicio de alojamiento web de su  elección. El sitio web debe resumir  en el caso del proyecto, los tópicos del tema elegido , incorporar algunas  visualizaciones y  un  screencast al sitio web. Puedes ver un ejemplo de presentación parcial [aquí](http://matthewrocklin.com/blog/work/2017/02/11/dask-tensorflow).

#### Screencast

Cada equipo creará un screencast de 5 minutos como mínimo  mostrando, en que consiste su proyecto, utilizando un cuaderno de jupyter notebook  o algunas diapositivas. Puedes encontrar información de como hacerlo en [Making a Screencast Video](https://docs.google.com/document/d/1alPLuBOW5YPoQDa57KZes1h72PoQDoDj21-UEKOHp1I/pub) o en [How to record screencasts with recordMyDesktop](https://opensource.com/business/15/11/how-record-screencasts).

#### Referencias

* [Presentation Zen-How to Design & Deliver Presentations Like a Pro](http://www.garrreynolds.com/Presentation/pdf/presentation_tips.pdf).
* [Points of view: Storytelling](http://www.nature.com/nmeth/journal/v10/n8/full/nmeth.2571.html).


### Asignaciones 

* [Asignación1](https://nbviewer.jupyter.org/github/C-Lara/M-L/blob/master/AsignacionesML/Asignacion1/AsignacionML1.ipynb).
* [Asignación2](https://nbviewer.jupyter.org/github/C-Lara/M-L/blob/master/AsignacionesML/Asignacion2/AsignacionML2.ipynb).
* [Asignación3](https://nbviewer.jupyter.org/github/C-Lara/M-L/blob/master/AsignacionesML/Asignacion3/Asignacion3ML.ipynb).

### Horario de clases 

* Miércoles 4 - 6 Sala 2.
* Miércoles 6-  8 Sala 1.
 
 
## Herramientas a  usar 

### Anaconda 

[Anaconda](https://www.continuum.io/downloads) es una distribución completa  libre de [Python](https://www.python.org/) incluye [paquetes de Python ](http://docs.continuum.io/anaconda/pkg-docs).

Anaconda incluye los instaladores de Python 2.7 y 3.5.  La instalación en **Linux**, se encuentra en la página de Anaconda y es más o menos así

1 . Descargar el instalador de Anaconda para Linux.

2 . Después de descargar el instalar, en el terminal, ejecuta para 3.5

```bash
c-lara@Lara:~$ bash Anaconda3-2.4.1-Linux-x86_64.sh

```
Es recomendable leer, alguna de las característica de Anaconda en el siguiente material [conda 30-minutes test drive](http://conda.pydata.org/docs/test-drive.html).

3 . La instalación de paquetes como [seaborn](http://stanford.edu/~mwaskom/software/seaborn/) o [bokeh](http://bokeh.pydata.org/en/latest/) se pueden realizar a través de Anaconda, de la siguiente manera:



``` bash
c-lara@Lara:~$ conda install bokeh
```

Alternativamente podemos desde PyPI usando **pip**:

```bash
c-lara@Lara:~$ pip install bokeh
``` 

El proyecto [Anaconda](https://www.continuum.io/downloads) ha creado [R Essentials](http://anaconda.org/r/r-essentials), que incluye el IRKernel y alrededor de 80 paquetes para análisis de datos, incluyendo `dplyr`, `shiny`, `ggplot2`,`caret`, etc. Para instalar **R Essentials** en un entorno de trabajo, hacemos

```bash
c-lara@Lara:~$ conda install -c r r-essentials
``` 

### Proyecto Jupyter y el Jupyter Nbviewer

El [Proyecto Jupyter](http://jupyter.org/)  es una aplicación web que te permite crear y compartir documentos que contienen código de diversos lenguajes de programación, ecuaciones,  visualizaciones y texto en diversos formatos. El uso de Jupyter incluye la ciencia de datos, simulación numérica, la modelización en estadística, Machine Learning, etc.


[Jupyter nbviewer](https://nbviewer.jupyter.org/)  es un servicio web gratuito que te permite compartir las versiones de archivos realizados por Jupyter, permitiendo el renderizado de diversos fórmatos incluyendo, código latex.

- [Jupyter Documentation](https://jupyter.readthedocs.io/en/latest/).


### Git y Github

[Git](https://git-scm.com/) es un sistema de control de versiones de gran potencia y versatilidad en el manejo de un gran número de archivos de  código fuente a a través del desarrollo no lineal, es decir vía la gestión rápida de ramas y mezclado de diferentes versiones.

Para poder revisar y aprender los comandos necesarios de Git, puedes darle una ojeada al excelente [tutorial de CodeSchool](https://try.github.io/levels/1/challenges/1) o a la [guía](http://rogerdudler.github.io/git-guide/index.es.html) de Roger Dudle para aprender  Git.

[Github](https://github.com/) es una plataforma de desarrollo colaborativo de software utilizado para alojar proyectos (muchos proyectos importantes como paquetes de R, Django, el Kernel de Linux, se encuentran alojados ahí) utilizando Git y el framework Ruby on Rails.

Podemos instalar Git en Ubuntu utilizando el administrador de paquetes `Apt`:

```bash
c-lara@Lara:~$sudo apt-get update
c-lara@Lara:~$sudo apt-get install git
```

Referencias y Lecturas

- [Usando el GIT](http://www.cs.swarthmore.edu/~newhall/unixhelp/git.php).
- [Practical Git Introduction](http://marc.helbling.fr/2014/09/practical-git-introduction).
- [Visual Git Guide](http://marklodato.github.io/visual-git-guide/index-es.html).

* Empezando con github clasroom

<iframe width="480" height="360" src="http://www.youtube.com/embed/ChA_zph7aao"> </iframe> 

* Manejando asignaciones grupales en github classroom

<iframe width="480" height="360" src="http://www.youtube.com/embed/-52quDR2QSc"> </iframe> 


### Scikit-learn 

[Scikit-learn](http://scikit-learn.org/stable/), es tal vez la mejor biblioteca para Machine Learning, construida sobre NumPy, SciPy y Matplotlib, esta biblioteca contiene una gran cantidad de herramientas eficientes para el Machine Learning y el modelado estadístico incluyendo clasificación, regresión, agrupación y reducción de la dimensionalidad.

- Video sobre [Machine Learning with scikit-learn](https://www.youtube.com/watch?v=HC0J_SPm9co) de Jake VanderPlas.
- [Machine Learning Cheat Sheet for scikit-learn](http://peekaboo-vision.blogspot.pe/2013/01/machine-learning-cheat-sheet-for-scikit.html).
- [Notas de Scikit-Learn como herramienta de Machine Learning](http://www.analyticsvidhya.com/blog/2015/01/scikit-learn-python-machine-learning-tool/).

* Videos de Jake VanderPlas sobre Machine Learning con  Scikit Learn :

<iframe width="480" height="360" src="http://www.youtube.com/embed/HC0J_SPm9co"> </iframe> 

### Otras herramientas

### Weka 

[Weka](http://www.cs.waikato.ac.nz/ml/weka/) (Waikato Environment for Knowledge Acquisition) es una herramienta de Machine Learning y Data Minning escrito en Java. Con Weka podemos hacer Preprocessing data, clustering, classification, regression y ahora Big Data y datos con un driver JDBC.

Para instalar Weka en Ubuntu, desempaquetar el [archivo](http://prdownloads.sourceforge.net/weka/weka-3-6-13.zip) conteniendo a Weka en algún de tu preferencia y luego ir al directorio creado  (weka-3-6-13) y ejecutar

```bash
c-lara@Lara:~$java -Xmx1000M -jar weka.jar
```

Podemos aprender un poco de Weka usando el [video](https://www.youtube.com/watch?v=m7kpIBGEdkI) de Brandon Weinberg o el [canal de Youtube](https://www.youtube.com/user/WekaMOOC) de Weka.

### Mahout 

[Mahout](http://mahout.apache.org/) es un proyecto que es es parte del proyecto Apache. Una característica principal de Mahout es su integración con el paradigma Hadoop Map/Reduce para el procesamiento de datos a gran escala. Mahout soporta un gran número de algoritmos incluyendo:

* Naive Bayes Classifier.
* K Means Clustering.
* Recommendation Engine.
* Logistic Regression Classifier.
* Random Forest.

Lecturas

-  Mahout in Action de Sean Owen, Robin Anil, Ted Dunning y Ellen Friedman, Manning Publications, 2011.

### Spark
[Spark](http://spark.apache.org/) es un framework de análisis distribuido en memoría y nos permite ir más allá de las operaciones en batch de Hadoop MapReduce: procesamiento de streaming, machine learning (MLlib), cálculo de grafos (GraphX), integración con lenguje R (Spark R) y análisis interactivos. 

Al igual que su predecesor, MapReduce  que  logra prácticamente una relación lineal de escalabilidad, Spark mantiene la escalabilidad lineal y la tolerancia a fallos de MapReduce, pero amplía sus bondades gracias a varias funcionalidades:

* DAG (Directed Acyclic Graph).
* RDD (Resilient Distributed Dataset).


Algunas lecturas y referencias


- [¿What is Apache Spark?](https://www.mapr.com/ebooks/spark/).
- [First steps with Spark](http://spark.apache.org/screencasts/1-first-steps-with-spark.html).
- [Spark Examples ](https://spark.apache.org/examples.html).
- [Apache Spark Videos](https://www.youtube.com/user/TheApacheSpark/videos).
- [Spark vs Hadoop](https://acadgild.com/blog/spark-vs-hadoop/).
